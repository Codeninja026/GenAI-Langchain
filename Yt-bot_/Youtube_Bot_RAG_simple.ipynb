{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c335591-a39b-4d74-a727-557cb079a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb53d78b-b50b-4abc-ab6a-efd0f02dac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec779ac-f97a-4e68-90c7-9648c1838625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a51ba08-2fad-4510-bd9a-f3d752b9cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f09e5ab3-3cb6-43a0-a336-76df68e24e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = 'openai/gpt-oss-20b',\n",
    "    task='text-generation',\n",
    "    huggingfacehub_api_token = api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b225644-ad30-4fca-8d13-22c1984c5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b31cb7ae-6c9a-4246-aff6-838d554e24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16dda118-99ee-4bb4-a35f-6ca107a32c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = 'https://www.youtube.com/watch?v=eMlx5fFNoYc'\n",
    "\n",
    "def get_text_url(url,lang='en'):\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"skip_download\":True,\n",
    "        \"writesubtitles\":True,\n",
    "        \"writeautomaticsub\":True,\n",
    "        'subtitleslangs':[lang],\n",
    "        'quiet':True\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as yts:\n",
    "        info = yts.extract_info(url,download=False)\n",
    "        subs = info.get('subtitles') or info.get('automatic updates')\n",
    "\n",
    "        if not subs or lang not in subs:\n",
    "            raise ValueError(f\"no sub found on lang {lang}\")\n",
    "        sub_url = subs[lang][0]['url']\n",
    "\n",
    "\n",
    "        res = requests.get(sub_url)\n",
    "\n",
    "        res.raise_for_status()\n",
    "        data = res.json() \n",
    "        text_segments = []\n",
    "\n",
    "        for event in data.get(\"events\", []):\n",
    "            if \"segs\" in event:\n",
    "                for seg in event[\"segs\"]:\n",
    "                    text_segments.append(seg.get(\"utf8\", \"\"))\n",
    "\n",
    "        return \" \".join(text_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f5e41de-37f4-4ad4-83a7-ece586729e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: ffmpeg not found. The downloaded format may not be the best available. Installing ffmpeg is strongly recommended: https://github.com/yt-dlp/yt-dlp#dependencies\n"
     ]
    }
   ],
   "source": [
    "text= get_text_url(video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c39db88-afe5-4b34-a96b-5d06655d5236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27820"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0075052c-a211-4e52-a43b-c575c6c91344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI. It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data. As a quick recap, here's the important context I want you to have in mind. The goal\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dec7a75f-fc95-44fd-9ed7-7a010b99efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fec962fc-9580-45e2-b6cf-9ee601f3d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7136139d-8fa6-462a-81ae-93fe281592b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec4f68b4-286a-4273-89cb-002e7f98c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "656ef57c-3ecc-4918-b1f6-6b3c3b9f95ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"let you do this. If you want to learn more about this stuff, I've left lots of links in the description. In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold. In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation. Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models. Thank you.\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fec2a47d-abed-4c1e-869d-9be620ec2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17f622e4-e402-4341-b972-67253a48bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41a3e45d-f23e-47ab-9a40-3060766247df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_store = FAISS.from_documents(chunks, model_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e01eff63-bc0c-4187-b4ea-af9ab7ecfce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '644997cc-73e4-41bf-82de-994ee9f553dd',\n",
       " 1: '23530458-c281-4409-85e1-682b1c43c52e',\n",
       " 2: 'd07c461b-3f57-4e71-9724-2228af866b88',\n",
       " 3: '36ac8408-9c9b-421f-9f2c-692c1f42e6b7',\n",
       " 4: 'e94665d2-e7fa-4f3f-9dd8-fcbf7312ac8e',\n",
       " 5: 'cd9943ed-0a0c-4fef-a4b7-5ba36b53f130',\n",
       " 6: '51630656-98b6-4f3f-a2f0-4c28681ff059',\n",
       " 7: 'd4608948-15e4-4507-9fc0-8d55135fb67a',\n",
       " 8: 'a6a26522-f602-43e1-bb4b-7ab783754322',\n",
       " 9: '0f99bd93-1a7c-4d25-8391-b426987ac15a',\n",
       " 10: 'ae4268a3-1fb3-47d8-a78a-6eb20e10647a',\n",
       " 11: 'd1e53817-2167-471a-bd38-7443325635f3',\n",
       " 12: 'f84c60de-2fa0-4619-9ee6-5c252ab5836c',\n",
       " 13: '61f0b5cd-3207-4c3c-bde5-1b2a934f02c9',\n",
       " 14: '22343457-1008-476f-8ef0-18ccbbdbe081',\n",
       " 15: '8baaca5f-727e-4634-9774-f69cf3bc4b00',\n",
       " 16: 'a41c8ad7-42c4-4231-bc4b-9a57ac831507',\n",
       " 17: 'c1c2a4e1-c9e8-4f8a-b267-9355cfbff942',\n",
       " 18: '25fdd3c3-f49b-4fea-b058-4c9afa3378b2',\n",
       " 19: '3ee76f6b-6957-453f-80df-17041da0d9d2',\n",
       " 20: 'f433862d-2e16-4d43-9142-550af6258d6f',\n",
       " 21: '0ce22b81-fa58-43eb-bbe6-a2b0a9a8891e',\n",
       " 22: '92efe8ce-f4d2-43a7-b368-62a810ea3d84',\n",
       " 23: '16c1e41d-3320-406d-a2e6-923dcc816a38',\n",
       " 24: '8b28751f-8f13-4da3-b5dd-e3d23ed28a07',\n",
       " 25: '01b165b6-f5aa-4517-81f5-1d3649e274c7',\n",
       " 26: 'accb1c56-901a-49f7-a13d-ef8b25339e88',\n",
       " 27: '1b01f66e-552b-4092-9a52-72ecf2450450',\n",
       " 28: 'c53c9269-a49c-4d97-b6c1-80224e7fd6c5',\n",
       " 29: '5e25222a-7d07-4e46-a708-8bef8bf983a7',\n",
       " 30: '97e05ebe-5839-472e-9540-7dafc7a86774',\n",
       " 31: '340bc2fd-7d74-499b-bafd-61d0fe5809f8',\n",
       " 32: '9f395901-6546-4f96-a85f-11d3d79584d0',\n",
       " 33: '2de516a7-025b-461e-9eaa-0e488e506d1d',\n",
       " 34: '08c1c2bb-abca-48ff-a395-8d42e5c82d24'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdb771a5-943f-4d3b-9fa0-f0d45f22672d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='08c1c2bb-abca-48ff-a395-8d42e5c82d24', metadata={}, page_content=\"let you do this. If you want to learn more about this stuff, I've left lots of links in the description. In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold. In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation. Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models. Thank you.\")]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_store.get_by_ids(['08c1c2bb-abca-48ff-a395-8d42e5c82d24'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2ca7e12-bb10-4902-9ff1-f2133614c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = vec_store.as_retriever(search_type='similarity',search_kwargs = {'k':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b592b5cf-50b0-49f1-b0d2-822b35f70380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='61f0b5cd-3207-4c3c-bde5-1b2a934f02c9', metadata={}, page_content=\"be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution. If you're coming in from the last chapter, you know what we need to do then. We compute a softmax along each one of these columns to normalize the values. In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values. At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top. We call this grid an attention pattern. Now if you look at the original transformer paper, there's a really compact way that they write this all down. Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices. This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of\"),\n",
       " Document(id='340bc2fd-7d74-499b-bafd-61d0fe5809f8', metadata={}, page_content=\"you, I've left an on-screen note about it. It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources. Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block. For one thing, it also goes through these other operations called multi-layer perceptrons. We'll talk more about those in the next chapter. And then it repeatedly goes through many many copies of both of these operations. What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings. The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that\"),\n",
       " Document(id='644997cc-73e4-41bf-82de-994ee9f553dd', metadata={}, page_content=\"In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI. It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data. As a quick recap, here's the important context I want you to have in mind. The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next. The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words. The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding. The most\"),\n",
       " Document(id='1b01f66e-552b-4092-9a52-72ecf2450450', metadata={}, page_content=\"of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings. And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token. As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps. GPT-3 for example uses 96 attention heads inside each block. Considering that each one is already a bit confusing, it's certainly a lot to hold in your head. Just to spell it all out very explicitly, this means you have 96 distinct key and\")]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ret.invoke('what is transformer?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d246efc6-e2eb-4e58-b20c-b550383a3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"openai/gpt-oss-20b\",       \n",
    "    task=\"text-generation\",             \n",
    "    huggingfacehub_api_token=api_key,   \n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c09c1785-cc8b-4aa7-8ce7-6b6e3236a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ChatHuggingFace(llm=f_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22426e42-757b-4a02-ae2c-2da76de72ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4888bf49-c143-4518-bd21-d9e2346be6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template = '''\n",
    "        answer only the provided transcript context.\n",
    "        If the context is insufficient, just say don't know \n",
    "        \n",
    "        {context},\n",
    "        Question: {question}\n",
    "        ''',\n",
    "    input_variables=['context','question']\n",
    "\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e13ea80-6766-41ff-b3e4-a32fcb489445",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is attension?'\n",
    "retrieved_docs = ret.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e35a8ce9-27fc-4b73-9fd9-35e9ca6e1843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0ce22b81-fa58-43eb-bbe6-a2b0a9a8891e', metadata={}, page_content=\"changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block. Zooming out, this whole process is what you would describe as a single head of attention. As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value. I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3. These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space. This gives us an additional 1.5 million or so parameters for each one. If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in\"),\n",
       " Document(id='8b28751f-8f13-4da3-b5dd-e3d23ed28a07', metadata={}, page_content=\"in most papers looks a little different. I'll talk about it in a minute. In my opinion, it tends to make things a little more conceptually confusing. To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation. Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head. As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention. This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription. A\"),\n",
       " Document(id='51630656-98b6-4f3f-a2f0-4c28681ff059', metadata={}, page_content=\"through the computations, though, let's take a much simpler example. Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest. And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns. What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel. Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context. Actually, that's not quite true. They also encode the position of the word. There's a lot more to say about the specific way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context. Let's go ahead and denote these embeddings with the letter e.\"),\n",
       " Document(id='9f395901-6546-4f96-a85f-11d3d79584d0', metadata={}, page_content=\"The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure. Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that. Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads. That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total. So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps. In the next\")]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84d0eddc-8c83-477d-850d-0464ddfe8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = '\\n\\n'.join(doc.page_content for doc in retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c2a6952-d9b5-42c4-8c53-a26acfa571a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = template.invoke({'context':context_text,'question':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e8c1fdd-7f2e-452f-b136-1ea2bfc52a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\n        answer only the provided transcript context.\\n        If the context is insufficient, just say don't know \\n\\n        changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block. Zooming out, this whole process is what you would describe as a single head of attention. As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value. I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3. These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space. This gives us an additional 1.5 million or so parameters for each one. If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in\\n\\nin most papers looks a little different. I'll talk about it in a minute. In my opinion, it tends to make things a little more conceptually confusing. To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation. Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head. As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention. This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription. A\\n\\nthrough the computations, though, let's take a much simpler example. Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest. And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns. What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel. Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context. Actually, that's not quite true. They also encode the position of the word. There's a lot more to say about the specific way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context. Let's go ahead and denote these embeddings with the letter e.\\n\\nThe further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure. Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that. Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads. That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total. So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps. In the next,\\n        Question: what is attension?\\n        \")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69b072b8-c3d1-412f-a3c1-4ef9182aebd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='According to the transcript, **attention** is the process by which embeddings are repeatedly updated to incorporate information from all other embeddings in the sequence.  \\n- Each *attention head* receives the current embeddings (vectors that encode word identity and position), and uses three learned matrices – **key**, **query**, and **value** – to compute a new, context‑refined embedding for each token.  \\n- The key and query matrices project the embeddings into a smaller “key/query space” (128‑dimensional in the example), while the value matrix maps them back into the full embedding dimension (12,288‑dimensional).  \\n- Multiple heads run in parallel, and the whole block of heads is repeated across many layers (e.g., 96 layers in GPT‑3), yielding a highly expressive, context‑aware representation of the input.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 281, 'prompt_tokens': 903, 'total_tokens': 1184}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--6875ff3a-41cf-42d4-a126-765259ce2ddc-0', usage_metadata={'input_tokens': 903, 'output_tokens': 281, 'total_tokens': 1184})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = model2.invoke(final_prompt)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40b7875e-39b2-445a-a911-7e736141f1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the transcript, **attention** is the process by which embeddings are repeatedly updated to incorporate information from all other embeddings in the sequence.  \\n- Each *attention head* receives the current embeddings (vectors that encode word identity and position), and uses three learned matrices – **key**, **query**, and **value** – to compute a new, context‑refined embedding for each token.  \\n- The key and query matrices project the embeddings into a smaller “key/query space” (128‑dimensional in the example), while the value matrix maps them back into the full embedding dimension (12,288‑dimensional).  \\n- Multiple heads run in parallel, and the whole block of heads is repeated across many layers (e.g., 96 layers in GPT‑3), yielding a highly expressive, context‑aware representation of the input.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc1f90-0640-4c93-bc60-ae0a17c12343",
   "metadata": {},
   "source": [
    "## Using Chains easy-one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fe4816d9-201e-4689-a4c5-518de2d7c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda,RunnableParallel,RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "146bdd4f-29c1-45b6-9118-083375aae84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "749cf952-972e-4ab4-97c3-dcf2db1bf99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98a13879-1729-4fb6-8e2d-61064894e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_doc(retrieved_docs):\n",
    "    context_text = '\\n\\n'.join(doc.page_content for doc in retrieved_docs)\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "02f4bb63-bee1-44c7-8bb9-c52650c1f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "    'context': ret | RunnableLambda(merged_doc),\n",
    "    'question': RunnablePassthrough()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd1fb34c-734f-4842-af2c-1fea5f53e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = parallel_chain | template | model2 | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b5d24823-bb30-46ae-8bb9-8f403e1519b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.invoke('what is attension?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6e52a39d-b0ee-4b7f-a988-beb9d3e28402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attention is the mechanism inside a transformer that lets each token “look at” the other tokens in the sequence.  \\nIt works by projecting every input embedding into three matrices—**key**, **query** and **value**—and computing similarity scores between queries and keys. Those scores are turned into weights that mix the value vectors, producing a new, context‑aware representation for each position. In a single attention head this is parametrized by three learnable matrices; in a full transformer the block is a collection of many such heads running in parallel. The result is that embeddings become progressively richer, capturing relationships, sentiment, tone, etc., rather than just isolated word meaning.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
